{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /data/Yucong/anaconda3/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so\n",
      "CUDA SETUP: CUDA runtime path found: /data/Yucong/anaconda3/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /data/Yucong/anaconda3/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Yucong/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/data/Yucong/anaconda3/lib/libcudart.so.11.0'), PosixPath('/data/Yucong/anaconda3/lib/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "2023-09-30 13:03:28.232955: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='5'\n",
    "\n",
    "import fire\n",
    "# import gradio as gr\n",
    "import torch\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer, AutoTokenizer, AutoModelForCausalLM, AutoModel, TextStreamer\n",
    "\n",
    "# from utils.prompter import Prompter\n",
    "import bitsandbytes as bnb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_8bit = False\n",
    "base_model = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "model_path = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "model_name = 'LLaMA2'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c2bda697fe4f50bab5a089ec8ba7b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if model_name in ['ziya', 'Qwen', 'LLaMA2']:\n",
    "    print('Loading tokenizer...')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "\n",
    "    print('Loading model...')\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_8bit=load_8bit,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    if not load_8bit:\n",
    "        model.half()  # seems to fix bugs for some users.\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        model = torch.compile(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path as osp\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class Prompter_LLaMA2(object):\n",
    "    __slots__ = (\"template\", \"_verbose\")\n",
    "\n",
    "    def __init__(self, verbose: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: str,\n",
    "        input: Union[None, str] = None,\n",
    "    ) -> str:        \n",
    "        if input:\n",
    "            prompt = instruction + input\n",
    "        else:\n",
    "            prompt = instruction\n",
    "        system_message = \"Please give a proper response to the instruction. Do not say 'I don't know.\"\n",
    "        prompt_template=f'''[INST] <<SYS>>\n",
    "{system_message}\n",
    "<</SYS>>\n",
    "\n",
    "{prompt} [/INST]'''\n",
    "        \n",
    "        return prompt_template\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output.split('[/INST]')[-1].strip(tokenizer.eos_token).strip()\n",
    "\n",
    "prompter = Prompter_LLaMA2()\n",
    "\n",
    "def evaluate(\n",
    "    instruction,\n",
    "    input=None,\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=1,\n",
    "    max_new_tokens=4096,\n",
    "    **kwargs,\n",
    "):\n",
    "    \n",
    "    streamer = TextStreamer(tokenizer)\n",
    "    prompt = prompter.generate_prompt(instruction, input)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        num_beams=num_beams,\n",
    "        **kwargs,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            # streamer=streamer,\n",
    "        )\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s, skip_special_tokens=True)\n",
    "    return instruction, prompter.get_response(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! It's nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "instruction = 'hello'\n",
    "instruction, response = evaluate(instruction)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Sep 13:33    INFO  \n",
      "General Hyper Parameters:\n",
      "gpu_id = 6\n",
      "use_gpu = True\n",
      "seed = 2020\n",
      "state = INFO\n",
      "reproducibility = True\n",
      "data_path = /data/Yucong/anaconda3/lib/python3.9/site-packages/recbole/config/../dataset_example/ml-100k\n",
      "checkpoint_dir = experiments/ml-100k/LightGCN\n",
      "show_progress = True\n",
      "save_dataset = False\n",
      "dataset_save_path = None\n",
      "save_dataloaders = False\n",
      "dataloaders_save_path = None\n",
      "log_wandb = True\n",
      "\n",
      "Training Hyper Parameters:\n",
      "epochs = 500\n",
      "train_batch_size = 4096\n",
      "learner = adam\n",
      "learning_rate = 0.0005\n",
      "train_neg_sample_args = {'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}\n",
      "eval_step = 1\n",
      "stopping_step = 10\n",
      "clip_grad_norm = None\n",
      "weight_decay = 0.0\n",
      "loss_decimal_place = 4\n",
      "\n",
      "Evaluation Hyper Parameters:\n",
      "eval_args = {'group_by': 'user', 'order': 'TO', 'split': {'LS': 'valid_and_test'}, 'mode': 'full'}\n",
      "repeatable = False\n",
      "metrics = ['NDCG', 'Recall', 'MRR']\n",
      "topk = [10, 20]\n",
      "valid_metric = NDCG@10\n",
      "valid_metric_bigger = True\n",
      "eval_batch_size = 4096\n",
      "metric_decimal_place = 4\n",
      "\n",
      "Dataset Hyper Parameters:\n",
      "field_separator = \t\n",
      "seq_separator =  \n",
      "USER_ID_FIELD = user_id\n",
      "ITEM_ID_FIELD = item_id\n",
      "RATING_FIELD = rating\n",
      "TIME_FIELD = timestamp\n",
      "seq_len = None\n",
      "LABEL_FIELD = label\n",
      "threshold = None\n",
      "NEG_PREFIX = neg_\n",
      "load_col = {'inter': ['user_id', 'item_id', 'rating', 'timestamp']}\n",
      "unload_col = None\n",
      "unused_col = None\n",
      "additional_feat_suffix = None\n",
      "rm_dup_inter = None\n",
      "val_interval = None\n",
      "filter_inter_by_user_or_item = True\n",
      "user_inter_num_interval = None\n",
      "item_inter_num_interval = None\n",
      "alias_of_user_id = None\n",
      "alias_of_item_id = None\n",
      "alias_of_entity_id = None\n",
      "alias_of_relation_id = None\n",
      "preload_weight = None\n",
      "normalize_field = None\n",
      "normalize_all = True\n",
      "ITEM_LIST_LENGTH_FIELD = item_length\n",
      "LIST_SUFFIX = _list\n",
      "MAX_ITEM_LIST_LENGTH = 50\n",
      "POSITION_FIELD = position_id\n",
      "HEAD_ENTITY_ID_FIELD = head_id\n",
      "TAIL_ENTITY_ID_FIELD = tail_id\n",
      "RELATION_ID_FIELD = relation_id\n",
      "ENTITY_ID_FIELD = entity_id\n",
      "kg_reverse_r = False\n",
      "entity_kg_num_interval = None\n",
      "relation_kg_num_interval = None\n",
      "benchmark_filename = None\n",
      "\n",
      "Other Hyper Parameters: \n",
      "worker = 0\n",
      "wandb_project = RecBole\n",
      "shuffle = True\n",
      "require_pow = False\n",
      "enable_amp = False\n",
      "enable_scaler = False\n",
      "transform = None\n",
      "embedding_size = 32\n",
      "n_layers = 2\n",
      "reg_weight = 1e-05\n",
      "numerical_features = []\n",
      "discretization = None\n",
      "MODEL_TYPE = ModelType.GENERAL\n",
      "wandb_name = LightGCN-ml-100k\n",
      "MODEL_INPUT_TYPE = InputType.PAIRWISE\n",
      "eval_type = EvaluatorType.RANKING\n",
      "single_spec = True\n",
      "local_rank = 0\n",
      "device = cuda\n",
      "eval_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}\n",
      "\n",
      "\n",
      "30 Sep 13:33    INFO  ml-100k\n",
      "The number of users: 944\n",
      "Average actions of users: 106.04453870625663\n",
      "The number of items: 1683\n",
      "Average actions of items: 59.45303210463734\n",
      "The number of inters: 100000\n",
      "The sparsity of the dataset: 93.70575143257098%\n",
      "Remain Fields: ['user_id', 'item_id', 'rating', 'timestamp']\n",
      "30 Sep 13:33    INFO  [Training]: train_batch_size = [4096] train_neg_sample_args: [{'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}]\n",
      "30 Sep 13:33    INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'group_by': 'user', 'order': 'TO', 'split': {'LS': 'valid_and_test'}, 'mode': 'full'}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from recbole.quick_start import load_data_and_model\n",
    "dataset_name = 'ml-100k'\n",
    "rec_model_path = '../RecBole/experiments/ml-100k/LightGCN/LightGCN-Aug-28-2023_11-31-16.pth'\n",
    "\n",
    "config, rec_model, dataset, train_data, valid_data, test_data = load_data_and_model(\n",
    "    model_file=rec_model_path,\n",
    ")\n",
    "\n",
    "df_item = pd.read_csv(f'dataset/{dataset_name}/{dataset_name}.item', sep='\\t')\n",
    "df_inter = pd.read_csv(f'dataset/{dataset_name}/{dataset_name}.inter', sep='\\t')\n",
    "if 'ml' in dataset_name:\n",
    "    df_user = pd.read_csv(f'dataset/{dataset_name}/{dataset_name}.user', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 943/943 [00:00<00:00, 4440.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[196,\n",
       " 186,\n",
       " 22,\n",
       " 244,\n",
       " 166,\n",
       " 298,\n",
       " 115,\n",
       " 253,\n",
       " 305,\n",
       " 6,\n",
       " 62,\n",
       " 286,\n",
       " 200,\n",
       " 210,\n",
       " 224,\n",
       " 303,\n",
       " 122,\n",
       " 194,\n",
       " 291,\n",
       " 234,\n",
       " 119,\n",
       " 167,\n",
       " 299,\n",
       " 308,\n",
       " 95,\n",
       " 38,\n",
       " 102,\n",
       " 63,\n",
       " 160,\n",
       " 50,\n",
       " 301,\n",
       " 225,\n",
       " 290,\n",
       " 97,\n",
       " 157,\n",
       " 181,\n",
       " 278,\n",
       " 276,\n",
       " 7,\n",
       " 10,\n",
       " 284,\n",
       " 201,\n",
       " 287,\n",
       " 246,\n",
       " 242,\n",
       " 249,\n",
       " 99,\n",
       " 178,\n",
       " 251,\n",
       " 81,\n",
       " 260,\n",
       " 25,\n",
       " 59,\n",
       " 72,\n",
       " 87,\n",
       " 42,\n",
       " 292,\n",
       " 20,\n",
       " 13,\n",
       " 138,\n",
       " 60,\n",
       " 57,\n",
       " 223,\n",
       " 189,\n",
       " 243,\n",
       " 92,\n",
       " 241,\n",
       " 254,\n",
       " 293,\n",
       " 127,\n",
       " 222,\n",
       " 267,\n",
       " 11,\n",
       " 8,\n",
       " 162,\n",
       " 279,\n",
       " 145,\n",
       " 28,\n",
       " 135,\n",
       " 32,\n",
       " 90,\n",
       " 216,\n",
       " 250,\n",
       " 271,\n",
       " 265,\n",
       " 198,\n",
       " 168,\n",
       " 110,\n",
       " 58,\n",
       " 237,\n",
       " 94,\n",
       " 128,\n",
       " 44,\n",
       " 264,\n",
       " 41,\n",
       " 82,\n",
       " 262,\n",
       " 174,\n",
       " 43,\n",
       " 84,\n",
       " 269,\n",
       " 259,\n",
       " 85,\n",
       " 213,\n",
       " 121,\n",
       " 49,\n",
       " 155,\n",
       " 68,\n",
       " 172,\n",
       " 19,\n",
       " 268,\n",
       " 5,\n",
       " 80,\n",
       " 66,\n",
       " 18,\n",
       " 26,\n",
       " 130,\n",
       " 256,\n",
       " 1,\n",
       " 56,\n",
       " 15,\n",
       " 207,\n",
       " 232,\n",
       " 52,\n",
       " 161,\n",
       " 148,\n",
       " 125,\n",
       " 83,\n",
       " 272,\n",
       " 151,\n",
       " 54,\n",
       " 16,\n",
       " 91,\n",
       " 294,\n",
       " 229,\n",
       " 36,\n",
       " 70,\n",
       " 14,\n",
       " 295,\n",
       " 233,\n",
       " 214,\n",
       " 192,\n",
       " 100,\n",
       " 307,\n",
       " 297,\n",
       " 193,\n",
       " 113,\n",
       " 275,\n",
       " 219,\n",
       " 218,\n",
       " 123,\n",
       " 158,\n",
       " 302,\n",
       " 23,\n",
       " 296,\n",
       " 33,\n",
       " 154,\n",
       " 77,\n",
       " 270,\n",
       " 187,\n",
       " 170,\n",
       " 101,\n",
       " 184,\n",
       " 112,\n",
       " 133,\n",
       " 215,\n",
       " 69,\n",
       " 104,\n",
       " 240,\n",
       " 144,\n",
       " 191,\n",
       " 61,\n",
       " 142,\n",
       " 177,\n",
       " 203,\n",
       " 21,\n",
       " 197,\n",
       " 134,\n",
       " 180,\n",
       " 236,\n",
       " 263,\n",
       " 109,\n",
       " 64,\n",
       " 114,\n",
       " 239,\n",
       " 117,\n",
       " 65,\n",
       " 137,\n",
       " 257,\n",
       " 111,\n",
       " 285,\n",
       " 96,\n",
       " 116,\n",
       " 73,\n",
       " 221,\n",
       " 235,\n",
       " 164,\n",
       " 281,\n",
       " 182,\n",
       " 129,\n",
       " 45,\n",
       " 131,\n",
       " 230,\n",
       " 126,\n",
       " 231,\n",
       " 280,\n",
       " 288,\n",
       " 152,\n",
       " 217,\n",
       " 79,\n",
       " 75,\n",
       " 245,\n",
       " 282,\n",
       " 78,\n",
       " 118,\n",
       " 283,\n",
       " 171,\n",
       " 107,\n",
       " 226,\n",
       " 306,\n",
       " 173,\n",
       " 185,\n",
       " 150,\n",
       " 274,\n",
       " 188,\n",
       " 48,\n",
       " 311,\n",
       " 165,\n",
       " 208,\n",
       " 2,\n",
       " 205,\n",
       " 248,\n",
       " 93,\n",
       " 159,\n",
       " 146,\n",
       " 29,\n",
       " 156,\n",
       " 37,\n",
       " 141,\n",
       " 195,\n",
       " 108,\n",
       " 47,\n",
       " 255,\n",
       " 89,\n",
       " 140,\n",
       " 190,\n",
       " 24,\n",
       " 17,\n",
       " 313,\n",
       " 53,\n",
       " 124,\n",
       " 149,\n",
       " 176,\n",
       " 106,\n",
       " 312,\n",
       " 175,\n",
       " 153,\n",
       " 220,\n",
       " 143,\n",
       " 199,\n",
       " 202,\n",
       " 277,\n",
       " 206,\n",
       " 76,\n",
       " 314,\n",
       " 136,\n",
       " 179,\n",
       " 4,\n",
       " 304,\n",
       " 3,\n",
       " 227,\n",
       " 252,\n",
       " 212,\n",
       " 310,\n",
       " 35,\n",
       " 147,\n",
       " 105,\n",
       " 34,\n",
       " 71,\n",
       " 51,\n",
       " 204,\n",
       " 315,\n",
       " 31,\n",
       " 316,\n",
       " 103,\n",
       " 318,\n",
       " 30,\n",
       " 120,\n",
       " 46,\n",
       " 289,\n",
       " 209,\n",
       " 261,\n",
       " 88,\n",
       " 9,\n",
       " 247,\n",
       " 321,\n",
       " 266,\n",
       " 74,\n",
       " 238,\n",
       " 319,\n",
       " 323,\n",
       " 67,\n",
       " 211,\n",
       " 98,\n",
       " 12,\n",
       " 40,\n",
       " 258,\n",
       " 228,\n",
       " 325,\n",
       " 320,\n",
       " 326,\n",
       " 327,\n",
       " 183,\n",
       " 328,\n",
       " 322,\n",
       " 330,\n",
       " 27,\n",
       " 331,\n",
       " 332,\n",
       " 329,\n",
       " 86,\n",
       " 139,\n",
       " 300,\n",
       " 163,\n",
       " 333,\n",
       " 334,\n",
       " 39,\n",
       " 324,\n",
       " 132,\n",
       " 336,\n",
       " 335,\n",
       " 169,\n",
       " 338,\n",
       " 339,\n",
       " 309,\n",
       " 342,\n",
       " 340,\n",
       " 317,\n",
       " 341,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 273,\n",
       " 55,\n",
       " 349,\n",
       " 348,\n",
       " 354,\n",
       " 351,\n",
       " 358,\n",
       " 352,\n",
       " 360,\n",
       " 363,\n",
       " 355,\n",
       " 362,\n",
       " 357,\n",
       " 356,\n",
       " 361,\n",
       " 365,\n",
       " 350,\n",
       " 367,\n",
       " 368,\n",
       " 371,\n",
       " 373,\n",
       " 370,\n",
       " 374,\n",
       " 372,\n",
       " 337,\n",
       " 378,\n",
       " 366,\n",
       " 377,\n",
       " 375,\n",
       " 359,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 385,\n",
       " 382,\n",
       " 387,\n",
       " 364,\n",
       " 369,\n",
       " 388,\n",
       " 386,\n",
       " 389,\n",
       " 383,\n",
       " 390,\n",
       " 393,\n",
       " 392,\n",
       " 376,\n",
       " 394,\n",
       " 391,\n",
       " 398,\n",
       " 397,\n",
       " 399,\n",
       " 396,\n",
       " 401,\n",
       " 402,\n",
       " 384,\n",
       " 395,\n",
       " 353,\n",
       " 403,\n",
       " 405,\n",
       " 400,\n",
       " 406,\n",
       " 407,\n",
       " 409,\n",
       " 404,\n",
       " 413,\n",
       " 416,\n",
       " 408,\n",
       " 410,\n",
       " 411,\n",
       " 417,\n",
       " 412,\n",
       " 420,\n",
       " 422,\n",
       " 425,\n",
       " 419,\n",
       " 415,\n",
       " 423,\n",
       " 429,\n",
       " 428,\n",
       " 427,\n",
       " 418,\n",
       " 424,\n",
       " 432,\n",
       " 421,\n",
       " 435,\n",
       " 433,\n",
       " 426,\n",
       " 436,\n",
       " 430,\n",
       " 434,\n",
       " 437,\n",
       " 438,\n",
       " 431,\n",
       " 442,\n",
       " 440,\n",
       " 445,\n",
       " 447,\n",
       " 449,\n",
       " 450,\n",
       " 446,\n",
       " 439,\n",
       " 451,\n",
       " 452,\n",
       " 454,\n",
       " 453,\n",
       " 414,\n",
       " 455,\n",
       " 444,\n",
       " 448,\n",
       " 457,\n",
       " 456,\n",
       " 458,\n",
       " 462,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 467,\n",
       " 468,\n",
       " 466,\n",
       " 472,\n",
       " 465,\n",
       " 463,\n",
       " 471,\n",
       " 474,\n",
       " 469,\n",
       " 464,\n",
       " 476,\n",
       " 478,\n",
       " 473,\n",
       " 470,\n",
       " 480,\n",
       " 441,\n",
       " 479,\n",
       " 484,\n",
       " 486,\n",
       " 487,\n",
       " 482,\n",
       " 481,\n",
       " 492,\n",
       " 493,\n",
       " 490,\n",
       " 489,\n",
       " 483,\n",
       " 496,\n",
       " 494,\n",
       " 495,\n",
       " 477,\n",
       " 497,\n",
       " 488,\n",
       " 498,\n",
       " 499,\n",
       " 491,\n",
       " 500,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 443,\n",
       " 507,\n",
       " 514,\n",
       " 508,\n",
       " 511,\n",
       " 515,\n",
       " 512,\n",
       " 513,\n",
       " 475,\n",
       " 523,\n",
       " 518,\n",
       " 509,\n",
       " 516,\n",
       " 510,\n",
       " 524,\n",
       " 501,\n",
       " 525,\n",
       " 521,\n",
       " 520,\n",
       " 519,\n",
       " 528,\n",
       " 532,\n",
       " 530,\n",
       " 531,\n",
       " 529,\n",
       " 517,\n",
       " 527,\n",
       " 485,\n",
       " 533,\n",
       " 535,\n",
       " 536,\n",
       " 526,\n",
       " 537,\n",
       " 534,\n",
       " 541,\n",
       " 538,\n",
       " 542,\n",
       " 545,\n",
       " 539,\n",
       " 547,\n",
       " 543,\n",
       " 548,\n",
       " 546,\n",
       " 522,\n",
       " 551,\n",
       " 544,\n",
       " 553,\n",
       " 552,\n",
       " 540,\n",
       " 554,\n",
       " 550,\n",
       " 556,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 563,\n",
       " 566,\n",
       " 557,\n",
       " 558,\n",
       " 564,\n",
       " 565,\n",
       " 573,\n",
       " 549,\n",
       " 567,\n",
       " 569,\n",
       " 562,\n",
       " 576,\n",
       " 577,\n",
       " 579,\n",
       " 574,\n",
       " 555,\n",
       " 572,\n",
       " 575,\n",
       " 584,\n",
       " 588,\n",
       " 587,\n",
       " 568,\n",
       " 586,\n",
       " 585,\n",
       " 582,\n",
       " 591,\n",
       " 581,\n",
       " 592,\n",
       " 580,\n",
       " 590,\n",
       " 593,\n",
       " 583,\n",
       " 596,\n",
       " 570,\n",
       " 599,\n",
       " 589,\n",
       " 594,\n",
       " 597,\n",
       " 578,\n",
       " 601,\n",
       " 602,\n",
       " 600,\n",
       " 605,\n",
       " 603,\n",
       " 595,\n",
       " 606,\n",
       " 608,\n",
       " 607,\n",
       " 610,\n",
       " 611,\n",
       " 617,\n",
       " 618,\n",
       " 614,\n",
       " 609,\n",
       " 615,\n",
       " 616,\n",
       " 620,\n",
       " 571,\n",
       " 619,\n",
       " 613,\n",
       " 622,\n",
       " 621,\n",
       " 604,\n",
       " 624,\n",
       " 612,\n",
       " 627,\n",
       " 623,\n",
       " 628,\n",
       " 625,\n",
       " 629,\n",
       " 633,\n",
       " 632,\n",
       " 631,\n",
       " 634,\n",
       " 639,\n",
       " 630,\n",
       " 642,\n",
       " 637,\n",
       " 640,\n",
       " 626,\n",
       " 643,\n",
       " 598,\n",
       " 638,\n",
       " 635,\n",
       " 644,\n",
       " 636,\n",
       " 645,\n",
       " 648,\n",
       " 647,\n",
       " 650,\n",
       " 651,\n",
       " 654,\n",
       " 653,\n",
       " 655,\n",
       " 649,\n",
       " 658,\n",
       " 656,\n",
       " 660,\n",
       " 659,\n",
       " 646,\n",
       " 663,\n",
       " 664,\n",
       " 657,\n",
       " 665,\n",
       " 666,\n",
       " 661,\n",
       " 662,\n",
       " 667,\n",
       " 641,\n",
       " 668,\n",
       " 673,\n",
       " 671,\n",
       " 669,\n",
       " 676,\n",
       " 674,\n",
       " 652,\n",
       " 677,\n",
       " 682,\n",
       " 679,\n",
       " 684,\n",
       " 685,\n",
       " 683,\n",
       " 691,\n",
       " 672,\n",
       " 692,\n",
       " 690,\n",
       " 689,\n",
       " 686,\n",
       " 693,\n",
       " 688,\n",
       " 697,\n",
       " 698,\n",
       " 670,\n",
       " 694,\n",
       " 680,\n",
       " 705,\n",
       " 701,\n",
       " 699,\n",
       " 704,\n",
       " 707,\n",
       " 700,\n",
       " 687,\n",
       " 695,\n",
       " 675,\n",
       " 708,\n",
       " 709,\n",
       " 711,\n",
       " 710,\n",
       " 712,\n",
       " 715,\n",
       " 713,\n",
       " 716,\n",
       " 681,\n",
       " 678,\n",
       " 719,\n",
       " 702,\n",
       " 721,\n",
       " 714,\n",
       " 717,\n",
       " 718,\n",
       " 696,\n",
       " 722,\n",
       " 724,\n",
       " 727,\n",
       " 725,\n",
       " 706,\n",
       " 720,\n",
       " 729,\n",
       " 726,\n",
       " 728,\n",
       " 703,\n",
       " 738,\n",
       " 736,\n",
       " 734,\n",
       " 730,\n",
       " 743,\n",
       " 742,\n",
       " 737,\n",
       " 733,\n",
       " 745,\n",
       " 740,\n",
       " 735,\n",
       " 747,\n",
       " 723,\n",
       " 739,\n",
       " 749,\n",
       " 748,\n",
       " 746,\n",
       " 731,\n",
       " 750,\n",
       " 741,\n",
       " 751,\n",
       " 756,\n",
       " 757,\n",
       " 752,\n",
       " 758,\n",
       " 732,\n",
       " 762,\n",
       " 744,\n",
       " 754,\n",
       " 753,\n",
       " 763,\n",
       " 764,\n",
       " 767,\n",
       " 769,\n",
       " 755,\n",
       " 771,\n",
       " 768,\n",
       " 773,\n",
       " 765,\n",
       " 772,\n",
       " 766,\n",
       " 774,\n",
       " 760,\n",
       " 761,\n",
       " 777,\n",
       " 759,\n",
       " 776,\n",
       " 780,\n",
       " 779,\n",
       " 778,\n",
       " 782,\n",
       " 786,\n",
       " 784,\n",
       " 770,\n",
       " 788,\n",
       " 789,\n",
       " 790,\n",
       " 787,\n",
       " 783,\n",
       " 785,\n",
       " 794,\n",
       " 781,\n",
       " 796,\n",
       " 795,\n",
       " 793,\n",
       " 798,\n",
       " 791,\n",
       " 802,\n",
       " 800,\n",
       " 804,\n",
       " 803,\n",
       " 775,\n",
       " 792,\n",
       " 799,\n",
       " 805,\n",
       " 806,\n",
       " 807,\n",
       " 797,\n",
       " 801,\n",
       " 809,\n",
       " 815,\n",
       " 817,\n",
       " 821,\n",
       " 818,\n",
       " 814,\n",
       " 812,\n",
       " 823,\n",
       " 825,\n",
       " 827,\n",
       " 829,\n",
       " 811,\n",
       " 830,\n",
       " 826,\n",
       " 831,\n",
       " 819,\n",
       " 828,\n",
       " 808,\n",
       " 835,\n",
       " 833,\n",
       " 836,\n",
       " 816,\n",
       " 838,\n",
       " 839,\n",
       " 840,\n",
       " 832,\n",
       " 810,\n",
       " 844,\n",
       " 843,\n",
       " 834,\n",
       " 846,\n",
       " 837,\n",
       " 813,\n",
       " 842,\n",
       " 847,\n",
       " 848,\n",
       " 822,\n",
       " 852,\n",
       " 851,\n",
       " 849,\n",
       " 854,\n",
       " 850,\n",
       " 858,\n",
       " 853,\n",
       " 855,\n",
       " 824,\n",
       " 845,\n",
       " 841,\n",
       " 859,\n",
       " 862,\n",
       " 856,\n",
       " 820,\n",
       " 863,\n",
       " 860,\n",
       " 857,\n",
       " 864,\n",
       " 865,\n",
       " 868,\n",
       " 867,\n",
       " 861,\n",
       " 870,\n",
       " 871,\n",
       " 875,\n",
       " 876,\n",
       " 872,\n",
       " 866,\n",
       " 877,\n",
       " 873,\n",
       " 880,\n",
       " 878,\n",
       " 869,\n",
       " 881,\n",
       " 879,\n",
       " 883,\n",
       " 882,\n",
       " 884,\n",
       " 886,\n",
       " 885,\n",
       " 889,\n",
       " 874,\n",
       " 892,\n",
       " 890,\n",
       " 893,\n",
       " 887,\n",
       " 891,\n",
       " 894,\n",
       " 896,\n",
       " 897,\n",
       " 901,\n",
       " 899,\n",
       " 903,\n",
       " 904,\n",
       " 907,\n",
       " 905,\n",
       " 902,\n",
       " 898,\n",
       " 895,\n",
       " 906,\n",
       " 900,\n",
       " 908,\n",
       " 916,\n",
       " 911,\n",
       " 912,\n",
       " 914,\n",
       " 918,\n",
       " 919,\n",
       " 921,\n",
       " 910,\n",
       " 913,\n",
       " 915,\n",
       " 922,\n",
       " 923,\n",
       " 928,\n",
       " 927,\n",
       " 924,\n",
       " 929,\n",
       " 931,\n",
       " 917,\n",
       " 932,\n",
       " 909,\n",
       " 934,\n",
       " 933,\n",
       " 935,\n",
       " 938,\n",
       " 940,\n",
       " 888,\n",
       " 925,\n",
       " 942,\n",
       " 937,\n",
       " 926,\n",
       " 943,\n",
       " 939,\n",
       " 936,\n",
       " 930,\n",
       " 920,\n",
       " 941]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def get_valid_user_list(df):\n",
    "    valid_user_list = []\n",
    "    for id in tqdm(df['user_id:token'].unique().tolist()):\n",
    "        if len(df_inter[df_inter['user_id:token'] == id]) >=5:\n",
    "            valid_user_list.append(id)\n",
    "        if len(valid_user_list) == 1000:\n",
    "            break\n",
    "    return valid_user_list\n",
    "valid_user_list = get_valid_user_list(df_inter)\n",
    "valid_user_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recbole.utils.case_study import full_sort_topk\n",
    "def get_instruction_ml(idx):\n",
    "    # user_info\n",
    "    user_info = df_user[df_user[\"user_id:token\"] == int(idx)]\n",
    "    user_age = user_info[\"age:token\"].values[0]\n",
    "    user_gender = 'male' if user_info[\"gender:token\"].values[0] == 'M' else 'female'\n",
    "    user_occupation = user_info[\"occupation:token\"].values[0]\n",
    "\n",
    "    # history_inter_info\n",
    "    df_inter_user = df_inter[df_inter['user_id:token'] == int(idx)]\n",
    "    his_id_list = df_inter_user['item_id:token'].tolist()[-10:] \n",
    "    uid_series = dataset.token2id(dataset.uid_field, [idx])\n",
    "    topk_score, topk_iid_list = full_sort_topk(uid_series, rec_model, test_data, k=1, device=config['device'])\n",
    "    rec_list = dataset.id2token(dataset.iid_field, topk_iid_list.cpu()) \n",
    "\n",
    "    # next_item_info\n",
    "    next_item = df_item[df_item[\"item_id:token\"] == int(rec_list)]\n",
    "    next_item_title = next_item[\"movie_title:token_seq\"].values[0]\n",
    "    next_item_class = next_item[\"class:token_seq\"].values[0]\n",
    "\n",
    "    instruction = 'The history films watched by the customer are:\\n'\n",
    "    for i, id in enumerate(his_id_list):\n",
    "        instruction += f'{i+1}: {df_item[df_item[\"item_id:token\"] == id][\"movie_title:token_seq\"].values[0]}' + '\\n'\n",
    "        instruction += f'The class of the movie is {df_item[df_item[\"item_id:token\"] == id][\"class:token_seq\"].values[0]}.\\n'\n",
    "\n",
    "    # print(f\"The age of the customer is {user_age}, the gender is {user_gender} and the customer's occupation is {user_occupation}.\")\n",
    "\n",
    "\n",
    "    instruction += f\"\"\"\n",
    "The recommender system suggests the customer to watch this movie with the following title and class:\n",
    "{next_item_title}.\n",
    "The class of the movie is {next_item_class}.\n",
    "\n",
    "Your mission is to infer the customer's information from the history record, such as age, gender, occupation, etc.\n",
    "You must infer age, gender, and occupation. Do not return other information. And DO NOT return Unknow or Null.\n",
    "DO NOT RETURN OTHER INFORMATION!!! Only return the JSON format below!!!! So that the system can evaluate your answer.\n",
    "The output format is using JSON format as follows:\n",
    "{{\n",
    "    \"user age\": \"<accurate number of age>\",\n",
    "    \"user gender\": \"male or female\",\n",
    "    \"user occupation\": \"<occupation>\"\n",
    "}}\n",
    "\"\"\"\n",
    "    return instruction\n",
    "\n",
    "\n",
    "def get_instruction_mind(idx):\n",
    "    # history_inter_info\n",
    "    df_inter_user = df_inter[df_inter['user_id:token'] == int(idx)]\n",
    "    his_id_list = df_inter_user['item_id:token'].tolist()[-10:]\n",
    "    uid_series = dataset.token2id(dataset.uid_field, [idx])\n",
    "    topk_score, topk_iid_list = full_sort_topk(uid_series, rec_model, test_data, k=1, device=config['device'])\n",
    "    rec_list = dataset.id2token(dataset.iid_field, topk_iid_list.cpu()) \n",
    "    # next_item_info\n",
    "    next_item = df_item[df_item[\"item_id:token\"] == int(rec_list)]\n",
    "    next_item_title = next_item[\"title:token_seq\"].values[0]\n",
    "    next_item_category = next_item[\"category:token_seq\"].values[0]\n",
    "    next_item_sub_category = next_item[\"sub_category:token_seq\"].values[0]\n",
    "    next_item_abstract = next_item[\"abstract:token_seq\"].values[0]\n",
    "\n",
    "\n",
    "    instruction = 'The history news viewed by the customer are:\\n'\n",
    "    for i, id in enumerate(his_id_list):\n",
    "        item = df_item[df_item[\"item_id:token\"] == id]\n",
    "        item_title = item[\"title:token_seq\"].values[0]\n",
    "        item_category = item[\"category:token_seq\"].values[0]\n",
    "        item_sub_category = item[\"sub_category:token_seq\"].values[0]\n",
    "\n",
    "        instruction += f'{i+1}: {item_title}' + '\\n'\n",
    "        instruction += f'The category of the news is {item_category}.\\n'\n",
    "        instruction += f'The sub category of the news is {item_sub_category}.\\n'\n",
    "\n",
    "    instruction += f\"\"\"\n",
    "As a recommender system in the news domin, according to the user's previous news viewing history , \\\n",
    "tell the customer why he or she needs to watch this news.\n",
    "Give reasons why the customer needs to watch this news with the following title and category:\n",
    "{next_item_title}.\n",
    "The category of the news is {next_item_category}.\n",
    "The sub category of the news is {next_item_sub_category}.\n",
    "\n",
    "Use ONLY the information mentioned above, especially the history record. \\\n",
    "You're responding to the CUSTOMER. Directly tell the customer the reason. \\\n",
    "Respond in SHORT and CONCISE language. Your response should be WITHIN 75 words.\n",
    "\n",
    "\"\"\"\n",
    "    return instruction\n",
    "\n",
    "def get_instruction_steam(idx):\n",
    "    # history_inter_info\n",
    "    df_inter_user = df_inter[df_inter['user_id:token'] == int(idx)]\n",
    "    his_id_list = df_inter_user['product_id:token'].tolist()[-10:]\n",
    "    uid_series = dataset.token2id(dataset.uid_field, [idx])\n",
    "    topk_score, topk_iid_list = full_sort_topk(uid_series, rec_model, test_data, k=1, device=config['device'])\n",
    "    rec_list = dataset.id2token(dataset.iid_field, topk_iid_list.cpu()) \n",
    "    # next_item_info\n",
    "    next_item = df_item[df_item[\"id:token\"] == int(rec_list)]\n",
    "    next_item_title = next_item[\"app_name:token\"].values[0]\n",
    "    next_item_tag = next_item[\"genres:token_seq\"].values[0]\n",
    "    next_item_price = next_item[\"price:float\"].values[0]\n",
    "\n",
    "\n",
    "    instruction = 'The history games played by the customer are:\\n'\n",
    "    for i, id in enumerate(his_id_list):\n",
    "        item = df_item[df_item[\"id:token\"] == id]\n",
    "        item_title = item[\"app_name:token\"].values[0]\n",
    "        item_tag = item[\"genres:token_seq\"].values[0]\n",
    "\n",
    "        instruction += f'{i+1}: {item_title}' + '\\n'\n",
    "        instruction += f'The tags of the game are {item_tag}.\\n'\n",
    "\n",
    "    instruction += f\"\"\"\n",
    "As a recommender system in the game playing domain, according to the user's HISTORICAL play record , \\\n",
    "tell the customer why he or she needs to play this game, and what are the advantages of this game.\n",
    "Give reasons why the customer needs to play this game with the following title and tags:\n",
    "{next_item_title}.\n",
    "The tags of the game are {next_item_tag}.\n",
    "The price of the game is {next_item_price}.\n",
    "\n",
    "You can infer the game's information from the history record, such as price. \\\n",
    "Use ONLY the information mentioned above, especially the history record. \\\n",
    "You're responding to the CUSTOMER. Directly tell the customer the reason. \\\n",
    "Respond in SHORT and CONCISE language. Your response should be WITHIN 75 words.\n",
    "\n",
    "The output format is as follows:\n",
    "Game information: <only number of the price>\n",
    "Recommendation reason: <reason>\n",
    "\"\"\"\n",
    "    return instruction\n",
    "\n",
    "if dataset_name == 'ml-100k':\n",
    "    get_instruction = get_instruction_ml\n",
    "elif dataset_name == 'mind_small_dev':\n",
    "    get_instruction = get_instruction_mind\n",
    "elif dataset_name == 'steam':\n",
    "    get_instruction = get_instruction_steam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the history of movies watched by the customer, I can infer the following information about the customer:\n",
      "{\n",
      "    \"user age\": \"40-50\",\n",
      "    \"user gender\": \"male\",\n",
      "    \"user occupation\": \"entertainment enthusiast\"\n",
      "}\n",
      "\n",
      "Explanation:\n",
      "\n",
      "Based on the customer's history of watching a mix of action, adventure, drama, romance, sci-fi, and war movies, it is likely that the customer is a male in the age range of 40-50 who is interested in entertainment and enjoys a variety of genres.\n"
     ]
    }
   ],
   "source": [
    "instruction, response = evaluate(get_instruction(str(200)))\n",
    "# print(instruction)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user age': '40-50',\n",
       " 'user gender': 'male',\n",
       " 'user occupation': 'entertainment enthusiast'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "json_part = re.search(r'\\{(.+?)\\}', response, re.DOTALL).group()\n",
    "json_data = json.loads(json_part)\n",
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 137/943 [13:31<1:19:31,  5.92s/it, age_acc=0.303, gender_acc=0.727, occupation_acc=0.0202]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# random.shuffle(valid_user_list)\n",
    "\n",
    "# result_save_path = f'./results/LLaMA2/{dataset_name}/'\n",
    "# if not os.path.exists(result_save_path):\n",
    "#     os.makedirs(result_save_path)\n",
    "\n",
    "count = {}\n",
    "for key in ['age', 'gender', 'occupation']:\n",
    "    if key not in count.keys():\n",
    "        count.update({key: 0})\n",
    "acc = {}\n",
    "for key in ['age', 'gender', 'occupation']:\n",
    "    if key not in acc.keys():\n",
    "        acc.update({f'{key}_acc': 0})\n",
    "\n",
    "pbar = tqdm(valid_user_list[:1000])\n",
    "length = 0\n",
    "for id in pbar:\n",
    "    length += 1\n",
    "    if length == 100:\n",
    "        break\n",
    "    # user_info\n",
    "    user_info = df_user[df_user[\"user_id:token\"] == int(id)]\n",
    "    user_age = user_info[\"age:token\"].values[0]\n",
    "    user_gender = 'male' if user_info[\"gender:token\"].values[0] == 'M' else 'female'\n",
    "    user_occupation = user_info[\"occupation:token\"].values[0] \n",
    "\n",
    "    try:\n",
    "        # print(id)\n",
    "        instruction, response = evaluate(get_instruction_ml(str(id)))\n",
    "        json_part = re.search(r'\\{(.+?)\\}', response, re.DOTALL).group()\n",
    "        json_data = json.loads(json_part)\n",
    "        # print(json_data)\n",
    "        if abs(json_data['user age'] - user_age) <= 5:\n",
    "            count['age'] += 1\n",
    "        # print(json_data['user gender'], user_gender)\n",
    "        if json_data['user gender'] == user_gender:\n",
    "            count['gender'] += 1\n",
    "        if json_data['user occupation'] == user_occupation:\n",
    "            count['occupation'] += 1\n",
    "        for key in ['age', 'gender', 'occupation']:\n",
    "            acc.update({f'{key}_acc': count[key] / length})\n",
    "            pbar.set_postfix(acc, refresh=True)\n",
    "    except KeyboardInterrupt:\n",
    "        print('Interrupted by user')\n",
    "        break\n",
    "    except Exception:\n",
    "        length -= 1\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male male\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "id = 298\n",
    "user_info = df_user[df_user[\"user_id:token\"] == int(id)]\n",
    "user_age = user_info[\"age:token\"].values[0]\n",
    "user_gender = 'male' if user_info[\"gender:token\"].values[0] == 'M' else 'female'\n",
    "user_occupation = user_info[\"occupation:token\"].values[0] \n",
    "\n",
    "instruction, response = evaluate(get_instruction_ml(str(id)))\n",
    "json_part = re.search(r'\\{(.+?)\\}', response, re.DOTALL).group()\n",
    "json_data = json.loads(json_part)\n",
    "\n",
    "print(json_data['user gender'], user_gender)\n",
    "print(json_data['user gender'] == user_gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cal_item_pop(df_inter):\n",
    "    item_pop = {}\n",
    "    for i in range(len(df_inter)):\n",
    "        item_id = df_inter.iloc[i]['item_id:token']\n",
    "        if item_id in item_pop:\n",
    "            item_pop[item_id] += 1\n",
    "        else:\n",
    "            item_pop[item_id] = 1\n",
    "    return item_pop\n",
    "\n",
    "def visualize_item_pop(item_pop):\n",
    "    plt.hist(item_pop.values(), bins=10)\n",
    "    plt.show()\n",
    "\n",
    "item_pop = cal_item_pop(df_inter)\n",
    "visualize_item_pop(item_pop)\n",
    "\n",
    "for key, value in item_pop.items():\n",
    "    if value < 50:\n",
    "        item_pop[key] = 1\n",
    "    elif value < 100:\n",
    "        item_pop[key] = 2\n",
    "    elif value < 150:\n",
    "        item_pop[key] = 3\n",
    "    elif value < 200:\n",
    "        item_pop[key] = 4\n",
    "    elif value >= 200:\n",
    "        item_pop[key] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recbole.utils.case_study import full_sort_topk\n",
    "\n",
    "def get_instruction_ml_popularity(idx):\n",
    "    # user_info\n",
    "    user_info = df_user[df_user[\"user_id:token\"] == int(idx)]\n",
    "    user_age = user_info[\"age:token\"].values[0]\n",
    "    user_gender = 'male' if user_info[\"gender:token\"].values[0] == 'M' else 'female'\n",
    "    user_occupation = user_info[\"occupation:token\"].values[0]\n",
    "\n",
    "    # history_inter_info\n",
    "    df_inter_user = df_inter[df_inter['user_id:token'] == int(idx)]\n",
    "    his_id_list = df_inter_user['item_id:token'].tolist()[-10:] \n",
    "    uid_series = dataset.token2id(dataset.uid_field, [idx])\n",
    "    topk_score, topk_iid_list = full_sort_topk(uid_series, rec_model, test_data, k=1, device=config['device'])\n",
    "    rec_list = dataset.id2token(dataset.iid_field, topk_iid_list.cpu()) \n",
    "\n",
    "    # next_item_info\n",
    "    next_item = df_item[df_item[\"item_id:token\"] == int(rec_list)]\n",
    "    next_item_title = next_item[\"movie_title:token_seq\"].values[0]\n",
    "    next_item_class = next_item[\"class:token_seq\"].values[0]\n",
    "    \n",
    "    instruction = 'The popularity of the movie ranges from 1 to 5, 1 means the least popular and 5 means the most popular.\\n'\n",
    "    instruction += 'The history films watched by the customer are:\\n'\n",
    "    for i, id in enumerate(his_id_list):\n",
    "        instruction += f'{i+1}: {df_item[df_item[\"item_id:token\"] == id][\"movie_title:token_seq\"].values[0]}' + '\\n'\n",
    "        instruction += f'The class of the movie is {df_item[df_item[\"item_id:token\"] == id][\"class:token_seq\"].values[0]}. '\n",
    "        instruction += f'And the popularity of the movie is {item_pop[id]}.\\n'\n",
    "\n",
    "    instruction += f\"\"\"\n",
    "The recommender system suggests the customer to watch this movie with the following title and class:\n",
    "{next_item_title}.\n",
    "The class of the movie is {next_item_class}.\n",
    "\n",
    "Your mission is to infer the movie's popularity from the history record.\n",
    "You must infer popularity. Do not return other information. And DO NOT return Unknow or Null.\n",
    "DO NOT RETURN OTHER INFORMATION!!! Only return the JSON format below!!!! So that the system can evaluate your answer.\n",
    "The output format is using JSON format as follows:\n",
    "{{\n",
    "    \"popularity\": \"<accurate number of popularity in 1-5>\"\n",
    "}}\n",
    "\"\"\"\n",
    "    return instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"popularity\": 5,\n",
      "}\n",
      "\n",
      "Explanation:\n",
      "Based on the history record provided, the movie \"English Patient, The\" belongs to the class of Drama Romance War, which is consistent with the recommender system's suggestion. The popularity of the movie is 5, which is the highest level of popularity among all the movies listed in the history record.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'popularity': 5}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction, response = evaluate(get_instruction_ml_popularity(str(3)))\n",
    "# print(instruction)\n",
    "print(response)\n",
    "import json\n",
    "import re\n",
    "json_part = re.search(r'\\{(.+?)\\}', response, re.DOTALL).group()\n",
    "json_data = json.loads(json_part.replace(\",\", ''))\n",
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/943 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 100/943 [02:47<23:29,  1.67s/it, popularity_acc=0.747]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# random.shuffle(valid_user_list)\n",
    "\n",
    "# result_save_path = f'./results/LLaMA2/{dataset_name}/'\n",
    "# if not os.path.exists(result_save_path):\n",
    "#     os.makedirs(result_save_path)\n",
    "\n",
    "count = 0\n",
    "pbar = tqdm(valid_user_list[:1000])\n",
    "length = 0\n",
    "for id in pbar:\n",
    "    length += 1\n",
    "    if length == 100:\n",
    "        break\n",
    "    # user_info\n",
    "    user_info = df_user[df_user[\"user_id:token\"] == int(id)]\n",
    "    user_age = user_info[\"age:token\"].values[0]\n",
    "    user_gender = 'male' if user_info[\"gender:token\"].values[0] == 'M' else 'female'\n",
    "    user_occupation = user_info[\"occupation:token\"].values[0] \n",
    "\n",
    "    df_inter_user = df_inter[df_inter['user_id:token'] == int(id)]\n",
    "    his_id_list = df_inter_user['item_id:token'].tolist()[-10:] \n",
    "    uid_series = dataset.token2id(dataset.uid_field, [str(id)])\n",
    "    topk_score, topk_iid_list = full_sort_topk(uid_series, rec_model, test_data, k=1, device=config['device'])\n",
    "    rec_list = dataset.id2token(dataset.iid_field, topk_iid_list.cpu()) \n",
    "    item_popularity = item_pop[int(rec_list)]\n",
    "    \n",
    "    try:\n",
    "    # print(id)\n",
    "        instruction, response = evaluate(get_instruction_ml_popularity(str(id)))\n",
    "        json_part = re.search(r'\\{(.+?)\\}', response, re.DOTALL).group()\n",
    "        json_data = json.loads(json_part.replace(\",\", '')) \n",
    "        popularity = int(json_data['popularity'])\n",
    "        # print(popularity, item_popularity)\n",
    "        if popularity == item_popularity:\n",
    "            count += 1\n",
    "        pbar.set_postfix({'popularity_acc': count / length}, refresh=True)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('Interrupted by user')\n",
    "        break\n",
    "    except Exception:\n",
    "        length -= 1\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}