{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import json\n",
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                         instruction  \\\n0  The history games played by the customer are:\\...   \n1  The history games played by the customer are:\\...   \n2  The history games played by the customer are:\\...   \n\n                                              LLaMA2  \\\n0  Based on your historical play record, Rust is ...   \n1  Based on your historical play record, Rust is ...   \n2  Based on your historical play record, Team For...   \n\n                                            ChatGLM2  \\\n0  I'm sorry, but I am unable to determine the cu...   \n1  I'm sorry, but I am unable to determine the cu...   \n2  You play Team Fortress 2 because it is an acti...   \n\n                                              GPT3.5  \\\n0  Dear customer, you should play Rust because it...   \n1  Based on your historical play record, you shou...   \n2  Dear customer, you should play Team Fortress 2...   \n\n                                                GPT4  \\\n0  Based on your history, you enjoy action and ad...   \n1  You've shown interest in Action, Adventure, In...   \n2  Given your history, you'd enjoy Team Fortress ...   \n\n                                          LLaMA2-SFT  \n0  Based on your history, you enjoy action, adven...  \n1  Based on your history, you enjoy action and ad...  \n2  Based on your history, you enjoy action games ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>instruction</th>\n      <th>LLaMA2</th>\n      <th>ChatGLM2</th>\n      <th>GPT3.5</th>\n      <th>GPT4</th>\n      <th>LLaMA2-SFT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The history games played by the customer are:\\...</td>\n      <td>Based on your historical play record, Rust is ...</td>\n      <td>I'm sorry, but I am unable to determine the cu...</td>\n      <td>Dear customer, you should play Rust because it...</td>\n      <td>Based on your history, you enjoy action and ad...</td>\n      <td>Based on your history, you enjoy action, adven...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The history games played by the customer are:\\...</td>\n      <td>Based on your historical play record, Rust is ...</td>\n      <td>I'm sorry, but I am unable to determine the cu...</td>\n      <td>Based on your historical play record, you shou...</td>\n      <td>You've shown interest in Action, Adventure, In...</td>\n      <td>Based on your history, you enjoy action and ad...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The history games played by the customer are:\\...</td>\n      <td>Based on your historical play record, Team For...</td>\n      <td>You play Team Fortress 2 because it is an acti...</td>\n      <td>Dear customer, you should play Team Fortress 2...</td>\n      <td>Given your history, you'd enjoy Team Fortress ...</td>\n      <td>Based on your history, you enjoy action games ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '..//gen_exp_5_model/results/all'\n",
    "data_path_steam = osp.join(data_path, 'steam.csv')\n",
    "data_path_ml = osp.join(data_path, 'ml-100k.csv')\n",
    "data_path_mind = osp.join(data_path, 'mind_small_dev.csv')\n",
    "\n",
    "data_steam = pd.read_csv(data_path_steam)\n",
    "data_ml = pd.read_csv(data_path_ml)\n",
    "data_mind = pd.read_csv(data_path_mind)\n",
    "data_steam.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path as osp\n",
    "from typing import Union\n",
    "\n",
    "def get_instruction(instruction, exp1, exp2):\n",
    "    prompt = \"-\"*20 + \"Instruction\" + \"-\"*20 + \"\\n\"\n",
    "    prompt += instruction + \"\\n\"\n",
    "    prompt += \"-\"*20 + \"Explanation 1\" + \"-\"*20 + \"\\n\"\n",
    "    prompt += exp1 + \"\\n\"\n",
    "    prompt += \"-\"*20 + \"Explanation 2\" + \"-\"*20 + \"\\n\"\n",
    "    prompt += exp2 + \"\\n\"\n",
    "    prompt += \"-\"*53 + \"\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "class Prompter_GPT(object):\n",
    "    __slots__ = (\"template\", \"_verbose\")\n",
    "\n",
    "    def __init__(self, verbose: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: str,\n",
    "        input: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "        if input:\n",
    "            prompt = instruction + input\n",
    "        else:\n",
    "            prompt = instruction\n",
    "\n",
    "        prompt_template = \\\n",
    "f\"You are a discriminator that judges whether the explainability of the recommendation system is good or bad. You should judge which of the 2 interpretability opinions generated based on the following Instruction is better. Return 1 if you think the first one is better, and 2 if you think the second one is better. Only the number 1 or 2 should be returned. Do not return any other characters.\\n\\n{prompt}Based on the above instructions, decide which explanation better explains why the recommendation system recommends this item to the customer.Please return 1 or 2 to show your choice. Only return 1 or 2. Do not return any other information.\"\n",
    "\n",
    "        return prompt_template\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output\n",
    "\n",
    "prompter = Prompter_GPT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def gen_compare_test_sft_data(df, model1, model2, f):\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        data = df.iloc[i]\n",
    "        instruction = data['instruction']\n",
    "        exp1 = data[model1]\n",
    "        exp2 = data[model2]\n",
    "        instruction = get_instruction(instruction, exp1, exp2)\n",
    "\n",
    "        json_data = {}\n",
    "        json_data['instruction'] = instruction\n",
    "        json_data['input'] = ''\n",
    "        json_data['output'] = ''\n",
    "\n",
    "        json.dump(json_data, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "if not os.path.exists('data/'):\n",
    "    os.makedirs('data/steam/')\n",
    "    os.makedirs('data/ml/')\n",
    "    os.makedirs('data/mind/')\n",
    "    os.makedirs('data/compare_test/steam/')\n",
    "    os.makedirs('data/compare_test/ml/')\n",
    "    os.makedirs('data/compare_test/mind/')\n",
    "\n",
    "\n",
    "def gen_dataset_model_compare(dataset_name, model1, model2):\n",
    "    if dataset_name == 'mind':\n",
    "        df = data_mind\n",
    "    elif dataset_name == 'ml':\n",
    "        df = data_ml\n",
    "    elif dataset_name == 'steam':\n",
    "        df = data_steam\n",
    "    else:\n",
    "        raise ValueError('dataset_name should be one of mind, ml, steam')\n",
    "\n",
    "    with open(f'data/compare_test/{dataset_name}/{model1}-{model2}.json', 'w', encoding='utf-8') as f:\n",
    "        f.close()\n",
    "    with open(f'data/compare_test/{dataset_name}/{model1}-{model2}.json', 'a+', encoding='utf-8') as f:\n",
    "        gen_compare_test_sft_data(df, model1, model2, f)\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Generate MIND data for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 15997.93it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 12718.61it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 15898.35it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 12797.85it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 15894.32it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 15960.49it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 21316.96it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 15886.97it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 15889.02it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 15992.19it/s]\n"
     ]
    }
   ],
   "source": [
    "for model1, model2 in[('LLaMA2', 'ChatGLM2'),\n",
    "                      ('LLaMA2', 'GPT3.5'),\n",
    "                      ('LLaMA2', 'GPT4'),\n",
    "                      ('LLaMA2', 'LLaMA2-SFT'),\n",
    "                      ('ChatGLM2', 'GPT3.5'),\n",
    "                      ('ChatGLM2', 'GPT4'),\n",
    "                      ('ChatGLM2', 'LLaMA2-SFT'),\n",
    "                      ('GPT3.5', 'GPT4'),\n",
    "                      ('GPT3.5', 'LLaMA2-SFT'),\n",
    "                      ('GPT4', 'LLaMA2-SFT')]:\n",
    "    gen_dataset_model_compare('mind', model1, model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Generate ML data for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 943/943 [00:00<00:00, 12005.19it/s]\n",
      "100%|██████████| 943/943 [00:00<00:00, 15087.60it/s]\n",
      "100%|██████████| 943/943 [00:00<00:00, 15725.12it/s]\n",
      "100%|██████████| 943/943 [00:00<00:00, 12015.43it/s]\n",
      "100%|██████████| 943/943 [00:00<00:00, 11975.09it/s]\n",
      "100%|██████████| 943/943 [00:00<00:00, 15110.54it/s]\n",
      "100%|██████████| 943/943 [00:00<00:00, 11854.82it/s]\n",
      "100%|██████████| 943/943 [00:00<00:00, 12028.00it/s]\n",
      "100%|██████████| 943/943 [00:00<00:00, 12005.55it/s]\n",
      "100%|██████████| 943/943 [00:00<00:00, 11075.50it/s]\n"
     ]
    }
   ],
   "source": [
    "for model1, model2 in [('LLaMA2', 'ChatGLM2'),\n",
    "                       ('LLaMA2', 'GPT3.5'),\n",
    "                       ('LLaMA2', 'GPT4'),\n",
    "                       ('LLaMA2', 'LLaMA2-SFT'),\n",
    "                       ('ChatGLM2', 'GPT3.5'),\n",
    "                       ('ChatGLM2', 'GPT4'),\n",
    "                       ('ChatGLM2', 'LLaMA2-SFT'),\n",
    "                       ('GPT3.5', 'GPT4'),\n",
    "                       ('GPT3.5', 'LLaMA2-SFT'),\n",
    "                       ('GPT4', 'LLaMA2-SFT')]:\n",
    "    gen_dataset_model_compare('ml', model1, model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Generate Steam data for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 15874.29it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 12793.60it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 16522.37it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 21421.37it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 21332.36it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 15881.14it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 14702.98it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 17425.66it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 15891.79it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 15999.63it/s]\n"
     ]
    }
   ],
   "source": [
    "for model1, model2 in [('LLaMA2', 'ChatGLM2'),\n",
    "                       ('LLaMA2', 'GPT3.5'),\n",
    "                       ('LLaMA2', 'GPT4'),\n",
    "                       ('LLaMA2', 'LLaMA2-SFT'),\n",
    "                       ('ChatGLM2', 'GPT3.5'),\n",
    "                       ('ChatGLM2', 'GPT4'),\n",
    "                       ('ChatGLM2', 'LLaMA2-SFT'),\n",
    "                       ('GPT3.5', 'GPT4'),\n",
    "                       ('GPT3.5', 'LLaMA2-SFT'),\n",
    "                       ('GPT4', 'LLaMA2-SFT')]:\n",
    "    gen_dataset_model_compare('steam', model1, model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml\n",
      "LLaMA2 ChatGLM2\n",
      "LLaMA2 GPT3.5\n",
      "LLaMA2 GPT4\n",
      "LLaMA2 LLaMA2-SFT\n",
      "ChatGLM2 GPT3.5\n",
      "ChatGLM2 GPT4\n",
      "ChatGLM2 LLaMA2-SFT\n",
      "GPT3.5 GPT4\n",
      "GPT3.5 LLaMA2-SFT\n",
      "GPT4 LLaMA2-SFT\n",
      "mind\n",
      "LLaMA2 ChatGLM2\n",
      "LLaMA2 GPT3.5\n",
      "LLaMA2 GPT4\n",
      "LLaMA2 LLaMA2-SFT\n",
      "ChatGLM2 GPT3.5\n",
      "ChatGLM2 GPT4\n",
      "ChatGLM2 LLaMA2-SFT\n",
      "GPT3.5 GPT4\n",
      "GPT3.5 LLaMA2-SFT\n",
      "GPT4 LLaMA2-SFT\n",
      "steam\n",
      "LLaMA2 ChatGLM2\n",
      "LLaMA2 GPT3.5\n",
      "LLaMA2 GPT4\n",
      "LLaMA2 LLaMA2-SFT\n",
      "ChatGLM2 GPT3.5\n",
      "ChatGLM2 GPT4\n",
      "ChatGLM2 LLaMA2-SFT\n",
      "GPT3.5 GPT4\n",
      "GPT3.5 LLaMA2-SFT\n",
      "GPT4 LLaMA2-SFT\n"
     ]
    }
   ],
   "source": [
    "dataset = ['ml', 'mind', 'steam']\n",
    "train_ratio = 0.8\n",
    "\n",
    "train_data = []\n",
    "df = pd.DataFrame(columns=['index', 'dataset', 'model1', 'model2'])\n",
    "index = 0\n",
    "\n",
    "for dataset_name in dataset:\n",
    "    print(dataset_name)\n",
    "    for model1, model2 in [('LLaMA2', 'ChatGLM2'),\n",
    "                           ('LLaMA2', 'GPT3.5'),\n",
    "                           ('LLaMA2', 'GPT4'),\n",
    "                           ('LLaMA2', 'LLaMA2-SFT'),\n",
    "                           ('ChatGLM2', 'GPT3.5'),\n",
    "                           ('ChatGLM2', 'GPT4'),\n",
    "                           ('ChatGLM2', 'LLaMA2-SFT'),\n",
    "                           ('GPT3.5', 'GPT4'),\n",
    "                           ('GPT3.5', 'LLaMA2-SFT'),\n",
    "                           ('GPT4', 'LLaMA2-SFT')]:\n",
    "        print(model1, model2)\n",
    "        with open(f'data/compare_test/{dataset_name}/{model1}-{model2}.json', 'r', encoding='utf-8') as f:\n",
    "            data = f.readlines()\n",
    "            for line in data[: int(train_ratio*len(data))]:\n",
    "                train_data.append(line)\n",
    "                index += 1\n",
    "                df.loc[index] = [index, dataset_name, model1, model2]\n",
    "            f.close()\n",
    "\n",
    "with open(f'data/compare_test/all/train.json', 'w', encoding='utf-8') as f:\n",
    "    f.writelines(train_data)\n",
    "    f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml\n",
      "LLaMA2 ChatGLM2\n",
      "LLaMA2 GPT3.5\n",
      "LLaMA2 GPT4\n",
      "LLaMA2 LLaMA2-SFT\n",
      "ChatGLM2 GPT3.5\n",
      "ChatGLM2 GPT4\n",
      "ChatGLM2 LLaMA2-SFT\n",
      "GPT3.5 GPT4\n",
      "GPT3.5 LLaMA2-SFT\n",
      "GPT4 LLaMA2-SFT\n",
      "mind\n",
      "LLaMA2 ChatGLM2\n",
      "LLaMA2 GPT3.5\n",
      "LLaMA2 GPT4\n",
      "LLaMA2 LLaMA2-SFT\n",
      "ChatGLM2 GPT3.5\n",
      "ChatGLM2 GPT4\n",
      "ChatGLM2 LLaMA2-SFT\n",
      "GPT3.5 GPT4\n",
      "GPT3.5 LLaMA2-SFT\n",
      "GPT4 LLaMA2-SFT\n",
      "steam\n",
      "LLaMA2 ChatGLM2\n",
      "LLaMA2 GPT3.5\n",
      "LLaMA2 GPT4\n",
      "LLaMA2 LLaMA2-SFT\n",
      "ChatGLM2 GPT3.5\n",
      "ChatGLM2 GPT4\n",
      "ChatGLM2 LLaMA2-SFT\n",
      "GPT3.5 GPT4\n",
      "GPT3.5 LLaMA2-SFT\n",
      "GPT4 LLaMA2-SFT\n"
     ]
    }
   ],
   "source": [
    "dataset = ['ml', 'mind', 'steam']\n",
    "train_ratio = 0.8\n",
    "\n",
    "test_data = []\n",
    "df = pd.DataFrame(columns=['index', 'dataset', 'model1', 'model2'])\n",
    "index = 0\n",
    "\n",
    "for dataset_name in dataset:\n",
    "    print(dataset_name)\n",
    "    for model1, model2 in [('LLaMA2', 'ChatGLM2'),\n",
    "                           ('LLaMA2', 'GPT3.5'),\n",
    "                           ('LLaMA2', 'GPT4'),\n",
    "                           ('LLaMA2', 'LLaMA2-SFT'),\n",
    "                           ('ChatGLM2', 'GPT3.5'),\n",
    "                           ('ChatGLM2', 'GPT4'),\n",
    "                           ('ChatGLM2', 'LLaMA2-SFT'),\n",
    "                           ('GPT3.5', 'GPT4'),\n",
    "                           ('GPT3.5', 'LLaMA2-SFT'),\n",
    "                           ('GPT4', 'LLaMA2-SFT')]:\n",
    "        print(model1, model2)\n",
    "        with open(f'data/compare_test/{dataset_name}/{model1}-{model2}.json', 'r', encoding='utf-8') as f:\n",
    "            data = f.readlines()\n",
    "            for line in data[int(train_ratio*len(data)):]:\n",
    "                test_data.append(line)\n",
    "                index += 1\n",
    "                df.loc[index] = [index, dataset_name, model1, model2]\n",
    "            f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('data/compare_test/all/test_index.csv', index=False)\n",
    "with open(f'data/compare_test/all/test.json', 'w', encoding='utf-8') as f:\n",
    "    f.writelines(test_data)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}