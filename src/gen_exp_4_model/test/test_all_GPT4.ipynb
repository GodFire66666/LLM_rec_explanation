{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import json\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-4\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.5, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided user corpus does not contain any grammatical errors or misspellings. Therefore, the correction and status are as follows:\n",
      "\n",
      "{\n",
      "\"Misspellings\": [],\n",
      "\"Grammatical errors\": [],\n",
      "\"the corrected sentence\": \"The mouseleave event\",\n",
      "\"status\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# text = f\"\"\"\n",
    "# The mouseleave event\n",
    "# \"\"\"\n",
    "# prompt = f\"\"\"\n",
    "# As an expert in grammar correction, find all the grammatical errors and misspellings in the following text and give the corrected sentence. If the user corpus is found to be incomplete or otherwise incorrect, return the status as False, else True. The user's corpus is delimited by triple dashes.\n",
    "#\n",
    "# User corpus:\n",
    "# ---\n",
    "# {text}\n",
    "# ---\n",
    "#\n",
    "# Use the following output format and use JSON format with the following keys:\n",
    "# Misspellings: <a python list of user's misspellings >\n",
    "# Grammatical errors: <a python list of user's grammatical errors>\n",
    "# the corrected sentence: <the correct sentences after fixing>\n",
    "# status: <boolean value means whether the modification is successful or not>\n",
    "#\n",
    "# Do not return other information.\n",
    "#\n",
    "# \"\"\"\n",
    "# response = get_completion(prompt)\n",
    "# print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name='gpt-4', temperature=0.9, request_timeout=60)\n",
    "text = '{text}'\n",
    "prompt = ChatPromptTemplate.from_template(text)\n",
    "chain = LLMChain(llm = llm, prompt = prompt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path as osp\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class Prompter_GPT(object):\n",
    "    __slots__ = (\"template\", \"_verbose\")\n",
    "\n",
    "    def __init__(self, verbose: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: str,\n",
    "        input: Union[None, str] = None,\n",
    "    ) -> str:        \n",
    "        if input:\n",
    "            prompt = instruction + input\n",
    "        else:\n",
    "            prompt = instruction\n",
    "        prompt_template = prompt\n",
    "        \n",
    "        return prompt_template\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output\n",
    "\n",
    "prompter = Prompter_GPT()\n",
    "\n",
    "def evaluate(\n",
    "    instruction,\n",
    "):\n",
    "    output =chain.run(prompter.generate_prompt(instruction))\n",
    "\n",
    "    return instruction, prompter.get_response(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "instruction = 'hello'\n",
    "instruction, response = evaluate(instruction)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "dataset_name = 'mind_small_dev'\n",
    "\n",
    "instruction_path = f'./results/LLaMA2/{dataset_name}/results.csv'\n",
    "instruction_df = pd.read_csv(instruction_path)\n",
    "instruction_df\n",
    "\n",
    "def get_instruction_ChatGPT(idx):\n",
    "    instruction = instruction_df.iloc[idx]['instruction']\n",
    "    return instruction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:16<04:08,  2.65s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "result_save_path = f'./results/GPT3.5/{dataset_name}/'\n",
    "if not os.path.exists(result_save_path):\n",
    "    os.makedirs(result_save_path)\n",
    "\n",
    "df_result = pd.DataFrame(columns=['instruction', 'response'])\n",
    "# i = 0\n",
    "with open(result_save_path + 'results.csv', 'w', encoding='UTF-8') as f:\n",
    "        for idx in tqdm(range(instruction_df.shape[0])):\n",
    "            instruction, response = evaluate(get_instruction_ChatGPT(idx))\n",
    "            df_result = df_result.append({'instruction': instruction, 'response': response}, ignore_index=True)\n",
    "            # print(instruction)\n",
    "            # print(response)\n",
    "            # break\n",
    "        df_result.to_csv(f, index=False)\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [13:08<00:00,  7.89s/it]\n",
      "100%|██████████| 100/100 [15:26<00:00,  9.27s/it]\n",
      "100%|██████████| 100/100 [14:24<00:00,  8.64s/it]\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in ['mind_small_dev', 'steam', 'ml-100k']:\n",
    "    instruction_path = f'./results/LLaMA2/{dataset_name}/results.csv'\n",
    "    instruction_df = pd.read_csv(instruction_path)\n",
    "    instruction_df\n",
    "\n",
    "    def get_instruction_ChatGPT(idx):\n",
    "        instruction = instruction_df.iloc[idx]['instruction']\n",
    "        return instruction\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    import random\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    result_save_path = f'./results/GPT3.5/{dataset_name}/'\n",
    "    if not os.path.exists(result_save_path):\n",
    "        os.makedirs(result_save_path)\n",
    "\n",
    "    df_result = pd.DataFrame(columns=['instruction', 'response'])\n",
    "    # i = 0\n",
    "    with open(result_save_path + 'results.csv', 'w', encoding='UTF-8') as f:\n",
    "            for idx in tqdm(range(instruction_df.shape[0])):\n",
    "                instruction, response = evaluate(get_instruction_ChatGPT(idx))\n",
    "                df_result = df_result.append({'instruction': instruction, 'response': response}, ignore_index=True)\n",
    "                # print(instruction)\n",
    "                # print(response)\n",
    "                # break\n",
    "            df_result.to_csv(f, index=False)\n",
    "    f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}