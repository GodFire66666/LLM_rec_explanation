{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001B[1m\u001B[37mTnT                     \u001B[m  Wed Sep 20 19:36:18 2023  \u001B[1m\u001B[30m515.86.01\u001B[m\n",
      "\u001B[36m[0]\u001B[m \u001B[34mNVIDIA A100-PCIE-40GB\u001B[m |\u001B[1m\u001B[31m 68°C\u001B[m, \u001B[1m\u001B[32m100 %\u001B[m | \u001B[36m\u001B[1m\u001B[33m 5363\u001B[m / \u001B[33m40960\u001B[m MB | \u001B[1m\u001B[30mhss0729\u001B[m(\u001B[33m1853M\u001B[m) \u001B[1m\u001B[30mlyhe\u001B[m(\u001B[33m2393M\u001B[m) \u001B[1m\u001B[30mghzhao\u001B[m(\u001B[33m551M\u001B[m)\n",
      "\u001B[36m[1]\u001B[m \u001B[34mNVIDIA A100-PCIE-40GB\u001B[m |\u001B[1m\u001B[31m 51°C\u001B[m, \u001B[1m\u001B[32m 68 %\u001B[m | \u001B[36m\u001B[1m\u001B[33m32258\u001B[m / \u001B[33m40960\u001B[m MB | \u001B[1m\u001B[30mliliz\u001B[m(\u001B[33m30815M\u001B[m)\n",
      "\u001B[36m[2]\u001B[m \u001B[34mNVIDIA A100-PCIE-40GB\u001B[m |\u001B[31m 35°C\u001B[m, \u001B[1m\u001B[32m 38 %\u001B[m | \u001B[36m\u001B[1m\u001B[33m39361\u001B[m / \u001B[33m40960\u001B[m MB | \u001B[1m\u001B[30mjiaxianyan\u001B[m(\u001B[33m38795M\u001B[m)\n",
      "\u001B[36m[3]\u001B[m \u001B[34mNVIDIA A100-PCIE-40GB\u001B[m |\u001B[31m 49°C\u001B[m, \u001B[1m\u001B[32m 37 %\u001B[m | \u001B[36m\u001B[1m\u001B[33m 4392\u001B[m / \u001B[33m40960\u001B[m MB | \u001B[1m\u001B[30mkangjf\u001B[m(\u001B[33m721M\u001B[m) \u001B[1m\u001B[30mghzhao\u001B[m(\u001B[33m3103M\u001B[m)\n",
      "\u001B[36m[4]\u001B[m \u001B[34mNVIDIA A100-PCIE-40GB\u001B[m |\u001B[31m 28°C\u001B[m, \u001B[32m  0 %\u001B[m | \u001B[36m\u001B[1m\u001B[33m  568\u001B[m / \u001B[33m40960\u001B[m MB |\n",
      "\u001B[36m[5]\u001B[m \u001B[34mNVIDIA A100-PCIE-40GB\u001B[m |\u001B[1m\u001B[31m 71°C\u001B[m, \u001B[1m\u001B[32m100 %\u001B[m | \u001B[36m\u001B[1m\u001B[33m36422\u001B[m / \u001B[33m40960\u001B[m MB | \u001B[1m\u001B[30mhss0729\u001B[m(\u001B[33m1931M\u001B[m) \u001B[1m\u001B[30mhss0729\u001B[m(\u001B[33m1969M\u001B[m) \u001B[1m\u001B[30mlvrui\u001B[m(\u001B[33m29561M\u001B[m) \u001B[1m\u001B[30mlyhe\u001B[m(\u001B[33m2393M\u001B[m)\n",
      "\u001B[36m[6]\u001B[m \u001B[34mNVIDIA A100-PCIE-40GB\u001B[m |\u001B[31m 29°C\u001B[m, \u001B[32m  0 %\u001B[m | \u001B[36m\u001B[1m\u001B[33m17887\u001B[m / \u001B[33m40960\u001B[m MB | \u001B[1m\u001B[30mYucong\u001B[m(\u001B[33m17319M\u001B[m)\n",
      "\u001B[36m[7]\u001B[m \u001B[34mNVIDIA A100-PCIE-40GB\u001B[m |\u001B[1m\u001B[31m 63°C\u001B[m, \u001B[1m\u001B[32m 96 %\u001B[m | \u001B[36m\u001B[1m\u001B[33m37591\u001B[m / \u001B[33m40960\u001B[m MB | \u001B[1m\u001B[30mghzhao\u001B[m(\u001B[33m37025M\u001B[m)\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4'\n",
    "\n",
    "import fire\n",
    "# import gradio as gr\n",
    "import torch\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer, AutoTokenizer, AutoModelForCausalLM, AutoModel, TextStreamer\n",
    "\n",
    "# from utils.prompter import Prompter\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "load_8bit = False\n",
    "model_name = 'SFT'\n",
    "\n",
    "if model_name == 'LLaMA2':\n",
    "    base_model = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "    model_path = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "elif model_name == 'ChatGLM2':\n",
    "    base_model = 'THUDM/chatglm2-6b'\n",
    "    model_path = 'THUDM/chatglm2-6b'\n",
    "elif model_name == 'SFT':\n",
    "    base_model = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "    model_path = './model_SFT'\n",
    "    lora_weights = model_path\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33df77cdbb104ad99f04a58bfc0795fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if model_name in ['SFT']:\n",
    "    print('Loading tokenizer...')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "\n",
    "    print('Loading model...')\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_8bit=load_8bit,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    print(f\"using lora {lora_weights}\")\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        lora_weights,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    \n",
    "    if not load_8bit:\n",
    "        model.half()  # seems to fix bugs for some users.\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path as osp\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class Prompter_LLaMA2(object):\n",
    "    __slots__ = (\"template\", \"_verbose\")\n",
    "\n",
    "    def __init__(self, verbose: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: str,\n",
    "        input: Union[None, str] = None,\n",
    "    ) -> str:        \n",
    "        if input:\n",
    "            prompt = instruction + input\n",
    "        else:\n",
    "            prompt = instruction\n",
    "        system_message = \"Please give a proper response to the instruction. Do not say 'I don't know.\"\n",
    "        prompt_template=f'''[INST] <<SYS>>\n",
    "{system_message}\n",
    "<</SYS>>\n",
    "\n",
    "{prompt} [/INST]'''\n",
    "        \n",
    "        return prompt_template\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output.split('[/INST]')[-1].strip(tokenizer.eos_token).strip()\n",
    "    \n",
    "class Prompter_ChatGLM2(object):\n",
    "    __slots__ = (\"template\", \"_verbose\")\n",
    "\n",
    "    def __init__(self, verbose: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: str,\n",
    "        input: Union[None, str] = None,\n",
    "    ) -> str:        \n",
    "        if input:\n",
    "            prompt = instruction + input\n",
    "        else:\n",
    "            prompt = instruction\n",
    "        \n",
    "        prompt_template = prompt \n",
    "        return prompt_template\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output.strip(tokenizer.eos_token).strip()\n",
    "\n",
    "prompter = Prompter_LLaMA2()\n",
    "\n",
    "def evaluate(\n",
    "    instruction,\n",
    "    input=None,\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=1,\n",
    "    max_new_tokens=4096,\n",
    "    **kwargs,\n",
    "):\n",
    "    \n",
    "    streamer = TextStreamer(tokenizer)\n",
    "    prompt = prompter.generate_prompt(instruction, input)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        num_beams=num_beams,\n",
    "        **kwargs,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            # streamer=streamer,\n",
    "        )\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s, skip_special_tokens=True)\n",
    "    return instruction, prompter.get_response(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "Given your interest in learning more about the world, I'd be happy to help you with your question. However, I must point out that the term \"hello\" is a common greeting used in many cultures and languages. It's not a specific term that can be associated with any particular country or region.\n",
      "If you have any specific information or context regarding the term \"hello,\" I'd be more than happy to help you with your query.\n"
     ]
    }
   ],
   "source": [
    "instruction = 'hello'\n",
    "instruction, response = evaluate(instruction)\n",
    "print(instruction)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [53:06<00:00,  3.19s/it]\n",
      "100%|██████████| 1000/1000 [1:07:41<00:00,  4.06s/it]\n",
      "100%|██████████| 943/943 [1:08:07<00:00,  4.33s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "for dataset_name in ['mind_small_dev','steam', 'ml-100k']:\n",
    "    instruction_path = f'../gen_exp_4_model/results/LLaMA2/{dataset_name}/results.csv'\n",
    "    instruction_df = pd.read_csv(instruction_path)\n",
    "    instruction_df\n",
    "\n",
    "    def get_instruction(idx):\n",
    "        instruction = instruction_df.iloc[idx]['instruction']\n",
    "        return instruction\n",
    "\n",
    "    result_save_path = f'./results/SFT/{dataset_name}/'\n",
    "    if not os.path.exists(result_save_path):\n",
    "        os.makedirs(result_save_path)\n",
    "\n",
    "    df_result = pd.DataFrame(columns=['instruction', 'response'])\n",
    "    # i = 0\n",
    "    with open(result_save_path + 'results.csv', 'w', encoding='UTF-8') as f:\n",
    "        for idx in tqdm(range(instruction_df.shape[0])):\n",
    "            instruction, response = evaluate(get_instruction(idx))\n",
    "            df_result = df_result.append({'instruction': instruction, 'response': response}, ignore_index=True)\n",
    "            # print(instruction)\n",
    "            # print(response)\n",
    "            # break\n",
    "        df_result.to_csv(f, index=False)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 将所有实验结果合并\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "origin_path = '../gen_exp_4_model/results/'\n",
    "SFT_path = './results/SFT/'\n",
    "for dataset_name in ['mind_small_dev','steam', 'ml-100k']:\n",
    "    df_LLaMA2 = pd.read_csv(origin_path + f'LLaMA2/{dataset_name}/results.csv')\n",
    "    df_ChatGLM2 = pd.read_csv(origin_path + f'ChatGLM2/{dataset_name}/results.csv')\n",
    "    df_GPT35 = pd.read_csv(origin_path + f'GPT3.5/{dataset_name}/results.csv')\n",
    "    df_GPT4 = pd.read_csv(origin_path + f'GPT4/{dataset_name}/results.csv')\n",
    "    df_SFT = pd.read_csv(SFT_path + f'{dataset_name}/results.csv')\n",
    "    df_all = pd.DataFrame(columns=['instruction', 'LLaMA2', 'ChatGLM2', 'GPT3.5', 'GPT4', 'LLaMA2-SFT'])\n",
    "    df_all['instruction'] = df_LLaMA2['instruction']\n",
    "    df_all['LLaMA2'] = df_LLaMA2['response']\n",
    "    df_all['ChatGLM2'] = df_ChatGLM2['response']\n",
    "    df_all['GPT3.5'] = df_GPT35['response']\n",
    "    df_all['GPT4'] = df_GPT4['response']\n",
    "    df_all['LLaMA2-SFT'] = df_SFT['response']\n",
    "\n",
    "    if not os.path.exists('./results/all/'):\n",
    "        os.makedirs('./results/all/')\n",
    "    df_all.to_csv('./results/all/' + f'{dataset_name}.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}